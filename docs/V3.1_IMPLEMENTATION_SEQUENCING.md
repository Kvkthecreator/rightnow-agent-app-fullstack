# YARNNN v3.1 Semantic Layer - Implementation Sequencing

**Architecture Reference**: [SEMANTIC_LAYER_INTEGRATION_DESIGN.md](./SEMANTIC_LAYER_INTEGRATION_DESIGN.md)
**Target Timeline**: 3 weeks
**Approach**: Incremental pillar upgrades with validation gates

---

## Implementation Philosophy

**Deliberate Integration, Not Bolt-On**

Each week delivers a complete pillar upgrade with immediate value:
- Week 1: P1 agents make smarter decisions (fewer duplicates)
- Week 2: P2 agents infer causal relationships (graph intelligence)
- Week 3: P3/P4 agents reason semantically (richer insights/documents)

**Validation Gates**: Each week ends with quantitative success metrics before proceeding.

---

## Week 1: Infrastructure + P1 Semantic Duplicate Detection

### Goal
P1 agents use semantic search to detect duplicates, reducing duplicate blocks by 30-40%.

### Database Schema (Day 1-2)

#### Migration: `20250116_semantic_layer_infrastructure.sql`

```sql
-- 1. Add embedding column to blocks table
ALTER TABLE public.blocks
ADD COLUMN embedding vector(1536);

-- 2. Create pgvector index for similarity search
CREATE INDEX blocks_embedding_idx ON public.blocks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- 3. Create substrate_relationships table
CREATE TABLE public.substrate_relationships (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Core fields
    from_block_id UUID NOT NULL REFERENCES public.blocks(id) ON DELETE CASCADE,
    to_block_id UUID NOT NULL REFERENCES public.blocks(id) ON DELETE CASCADE,
    relationship_type TEXT NOT NULL CHECK (relationship_type IN ('addresses', 'supports', 'contradicts', 'depends_on')),

    -- Confidence and provenance
    confidence_score DECIMAL(3,2) CHECK (confidence_score >= 0 AND confidence_score <= 1),
    inference_method TEXT CHECK (inference_method IN ('semantic_search', 'llm_verification', 'user_created')),

    -- Governance lifecycle
    state TEXT NOT NULL DEFAULT 'PROPOSED' CHECK (state IN ('PROPOSED', 'ACCEPTED', 'REJECTED')),

    -- Metadata
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_by UUID REFERENCES auth.users(id),

    -- Constraints
    CONSTRAINT no_self_reference CHECK (from_block_id != to_block_id),
    UNIQUE (from_block_id, to_block_id, relationship_type)
);

-- Indexes for relationship traversal
CREATE INDEX relationships_from_block_idx ON public.substrate_relationships(from_block_id);
CREATE INDEX relationships_to_block_idx ON public.substrate_relationships(to_block_id);
CREATE INDEX relationships_type_idx ON public.substrate_relationships(relationship_type);
CREATE INDEX relationships_state_idx ON public.substrate_relationships(state);

-- 4. RLS Policies
ALTER TABLE public.substrate_relationships ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view relationships in their workspace baskets"
ON public.substrate_relationships FOR SELECT
USING (
    EXISTS (
        SELECT 1 FROM public.blocks b
        JOIN public.baskets bsk ON b.basket_id = bsk.id
        WHERE b.id = substrate_relationships.from_block_id
        AND bsk.workspace_id IN (
            SELECT workspace_id FROM public.workspace_members
            WHERE user_id = auth.uid()
        )
    )
);

CREATE POLICY "Service role can manage all relationships"
ON public.substrate_relationships FOR ALL
USING (auth.jwt() ->> 'role' = 'service_role');
```

### Shared Primitives API (Day 2-3)

#### File: `api/src/services/semantic_primitives.py`

```python
"""
Semantic Layer Shared Primitives

Used by P1-P4 agents for consistent semantic operations.
"""

from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from supabase import Client
import openai
import os

# Configuration
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536

@dataclass
class SemanticSearchFilters:
    """Filters for semantic search"""
    semantic_types: Optional[List[str]] = None
    anchor_roles: Optional[List[str]] = None
    states: Optional[List[str]] = None
    min_similarity: float = 0.70

@dataclass
class BlockWithSimilarity:
    """Block with similarity score"""
    id: str
    basket_id: str
    text_content: str
    semantic_type: str
    anchor_role: Optional[str]
    state: str
    similarity_score: float
    metadata: Dict[str, Any]

@dataclass
class RelationshipProposal:
    """Proposed relationship between blocks"""
    from_block_id: str
    to_block_id: str
    relationship_type: str
    confidence_score: float
    inference_method: str
    reasoning: str

@dataclass
class BlockWithDepth:
    """Block with traversal depth"""
    id: str
    text_content: str
    semantic_type: str
    depth: int
    relationship_type: str

# ============================================================================
# Core Primitives
# ============================================================================

async def generate_embedding(text: str) -> List[float]:
    """
    Generate embedding for text using OpenAI API.

    Args:
        text: Text content to embed

    Returns:
        1536-dimensional embedding vector
    """
    openai.api_key = os.getenv("OPENAI_API_KEY")

    response = openai.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text
    )

    return response.data[0].embedding

async def semantic_search(
    supabase: Client,
    basket_id: str,
    query_text: str,
    filters: SemanticSearchFilters,
    limit: int = 20
) -> List[BlockWithSimilarity]:
    """
    Search for semantically similar blocks within a basket.

    Args:
        supabase: Supabase client
        basket_id: Basket to search within
        query_text: Text to find similar blocks for
        filters: Type/role/state filters
        limit: Maximum results to return

    Returns:
        List of blocks with similarity scores (sorted by similarity desc)
    """
    # Generate query embedding
    query_embedding = await generate_embedding(query_text)

    # Build filter conditions
    filter_conditions = ["basket_id = $1", "embedding IS NOT NULL"]
    params = [basket_id]
    param_index = 2

    if filters.semantic_types:
        filter_conditions.append(f"semantic_type = ANY($${param_index})")
        params.append(filters.semantic_types)
        param_index += 1

    if filters.anchor_roles:
        filter_conditions.append(f"anchor_role = ANY($${param_index})")
        params.append(filters.anchor_roles)
        param_index += 1

    if filters.states:
        filter_conditions.append(f"state = ANY($${param_index})")
        params.append(filters.states)
        param_index += 1

    where_clause = " AND ".join(filter_conditions)

    # Execute semantic search
    query = f"""
        SELECT
            id,
            basket_id,
            text_content,
            semantic_type,
            anchor_role,
            state,
            metadata,
            1 - (embedding <=> $${param_index}::vector) AS similarity_score
        FROM public.blocks
        WHERE {where_clause}
            AND 1 - (embedding <=> $${param_index}::vector) >= ${filters.min_similarity}
        ORDER BY embedding <=> $${param_index}::vector
        LIMIT {limit}
    """
    params.append(query_embedding)

    response = supabase.rpc('execute_sql', {'query': query, 'params': params}).execute()

    return [
        BlockWithSimilarity(
            id=row['id'],
            basket_id=row['basket_id'],
            text_content=row['text_content'],
            semantic_type=row['semantic_type'],
            anchor_role=row['anchor_role'],
            state=row['state'],
            similarity_score=row['similarity_score'],
            metadata=row['metadata']
        )
        for row in response.data
    ]

async def semantic_search_cross_basket(
    supabase: Client,
    workspace_id: str,
    query_text: str,
    scopes: List[str] = ['WORKSPACE', 'GLOBAL'],
    semantic_types: Optional[List[str]] = None,
    limit: int = 10
) -> List[BlockWithSimilarity]:
    """
    Search for semantically similar blocks across workspace scope.

    Args:
        supabase: Supabase client
        workspace_id: Workspace to search within
        query_text: Text to find similar blocks for
        scopes: Basket scopes to include
        semantic_types: Optional semantic type filter
        limit: Maximum results to return

    Returns:
        List of blocks with similarity scores (sorted by similarity desc)
    """
    query_embedding = await generate_embedding(query_text)

    # Build filter conditions
    filter_conditions = ["embedding IS NOT NULL"]
    params = []
    param_index = 1

    # Workspace + scope filter
    filter_conditions.append(f"""
        basket_id IN (
            SELECT id FROM public.baskets
            WHERE workspace_id = $${param_index}
            AND scope = ANY($${param_index + 1})
        )
    """)
    params.extend([workspace_id, scopes])
    param_index += 2

    if semantic_types:
        filter_conditions.append(f"semantic_type = ANY($${param_index})")
        params.append(semantic_types)
        param_index += 1

    where_clause = " AND ".join(filter_conditions)

    query = f"""
        SELECT
            id,
            basket_id,
            text_content,
            semantic_type,
            anchor_role,
            state,
            metadata,
            1 - (embedding <=> $${param_index}::vector) AS similarity_score
        FROM public.blocks
        WHERE {where_clause}
        ORDER BY embedding <=> $${param_index}::vector
        LIMIT {limit}
    """
    params.append(query_embedding)

    response = supabase.rpc('execute_sql', {'query': query, 'params': params}).execute()

    return [
        BlockWithSimilarity(
            id=row['id'],
            basket_id=row['basket_id'],
            text_content=row['text_content'],
            semantic_type=row['semantic_type'],
            anchor_role=row['anchor_role'],
            state=row['state'],
            similarity_score=row['similarity_score'],
            metadata=row['metadata']
        )
        for row in response.data
    ]

# Week 2 primitives (stub for now)
async def infer_relationships(
    supabase: Client,
    block_id: str,
    basket_id: str
) -> List[RelationshipProposal]:
    """Week 2: Infer causal relationships for a block"""
    raise NotImplementedError("Week 2")

# Week 3 primitives (stub for now)
async def traverse_relationships(
    supabase: Client,
    start_block_id: str,
    relationship_type: str,
    direction: str = 'forward',
    max_depth: int = 2
) -> List[BlockWithDepth]:
    """Week 3: Traverse relationship graph"""
    raise NotImplementedError("Week 3")
```

### P1 Agent Integration (Day 4-5)

#### File: `api/src/services/pillar1_evolution_agent.py`

**Changes:**

```python
from .semantic_primitives import (
    semantic_search,
    SemanticSearchFilters,
    generate_embedding
)

async def execute_evolution_phase(
    supabase: Client,
    basket_id: str,
    raw_dump_ids: List[str]
) -> Dict[str, Any]:
    """
    P1 Evolution Phase with V3.1 semantic duplicate detection.
    """
    # 1. Read raw dumps
    dumps = await fetch_raw_dumps(supabase, raw_dump_ids)

    # 2. Extract facts/insights (existing logic)
    extracted_facts = await extract_facts_from_dumps(dumps)

    # 3. V3.1: Semantic duplicate detection
    operations = []
    for fact in extracted_facts:
        # Search for semantically similar blocks
        similar_blocks = await semantic_search(
            supabase=supabase,
            basket_id=basket_id,
            query_text=fact.text,
            filters=SemanticSearchFilters(
                semantic_types=[fact.semantic_type],
                states=['ACCEPTED', 'ACTIVE'],
                min_similarity=0.70
            ),
            limit=5
        )

        # Determine operation based on similarity
        if similar_blocks and similar_blocks[0].similarity_score > 0.85:
            # High confidence duplicate → MERGE
            operations.append({
                'type': 'MERGE',
                'target_block_id': similar_blocks[0].id,
                'source_content': fact.text,
                'reasoning': f"High semantic similarity ({similar_blocks[0].similarity_score:.2f})"
            })
        elif similar_blocks and similar_blocks[0].similarity_score > 0.70:
            # Medium similarity → UPDATE/ENRICH
            operations.append({
                'type': 'UPDATE',
                'target_block_id': similar_blocks[0].id,
                'enrichment_content': fact.text,
                'reasoning': f"Related content ({similar_blocks[0].similarity_score:.2f})"
            })
        else:
            # Low similarity → CREATE
            operations.append({
                'type': 'CREATE',
                'content': fact.text,
                'semantic_type': fact.semantic_type,
                'reasoning': 'Novel content'
            })

    # 4. Create governance proposals (existing logic)
    proposals = await create_governance_proposals(supabase, basket_id, operations)

    # 5. Auto-approve high-confidence operations (existing logic)
    for proposal in proposals:
        if proposal['confidence_score'] > 0.90:
            await approve_proposal(supabase, proposal['id'])

    # 6. Execute approved proposals (existing logic)
    results = await execute_proposals(supabase, basket_id)

    # 7. V3.1: Generate embeddings for newly created blocks
    for result in results:
        if result['operation'] == 'CREATE':
            block_id = result['block_id']
            block = await fetch_block(supabase, block_id)

            # Generate and store embedding
            embedding = await generate_embedding(block['text_content'])
            await supabase.table('blocks').update({
                'embedding': embedding
            }).eq('id', block_id).execute()

    return {
        'operations_proposed': len(operations),
        'proposals_created': len(proposals),
        'proposals_executed': len(results),
        'semantic_duplicates_found': sum(1 for op in operations if op['type'] == 'MERGE')
    }
```

### Testing & Validation (Day 5)

#### Test: `tests/test_p1_semantic_duplicate_detection.py`

```python
import pytest
from api.src.services.pillar1_evolution_agent import execute_evolution_phase
from api.src.services.semantic_primitives import semantic_search

@pytest.mark.asyncio
async def test_semantic_duplicate_detection_merge(supabase_client, test_basket):
    """Test high similarity triggers MERGE operation"""

    # Setup: Create existing block
    existing_block = await create_block(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="User login flow needs rate limiting to prevent brute force attacks",
        semantic_type="constraint"
    )

    # Generate embedding for existing block
    embedding = await generate_embedding(existing_block['text_content'])
    await supabase_client.table('blocks').update({
        'embedding': embedding
    }).eq('id', existing_block['id']).execute()

    # Create raw dump with near-duplicate content
    raw_dump = await create_raw_dump(
        supabase_client,
        basket_id=test_basket['id'],
        content="Login system must implement rate limiting to stop brute force"
    )

    # Execute P1 evolution
    result = await execute_evolution_phase(
        supabase_client,
        basket_id=test_basket['id'],
        raw_dump_ids=[raw_dump['id']]
    )

    # Assert MERGE operation proposed
    assert result['semantic_duplicates_found'] == 1

    # Verify no duplicate block created
    blocks = await supabase_client.table('blocks').select('*').eq(
        'basket_id', test_basket['id']
    ).eq('semantic_type', 'constraint').execute()

    assert len(blocks.data) == 1  # Only original block exists

@pytest.mark.asyncio
async def test_semantic_search_with_filters(supabase_client, test_basket):
    """Test semantic search respects type filters"""

    # Setup: Create blocks of different types
    constraint_block = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="API rate limit is 1000 requests per hour",
        semantic_type="constraint"
    )

    fact_block = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Current API usage is 800 requests per hour",
        semantic_type="fact"
    )

    # Search for "API limit" with constraint filter
    results = await semantic_search(
        supabase=supabase_client,
        basket_id=test_basket['id'],
        query_text="What is the API request limit?",
        filters=SemanticSearchFilters(
            semantic_types=['constraint'],
            min_similarity=0.60
        )
    )

    # Assert only constraint block returned
    assert len(results) == 1
    assert results[0].id == constraint_block['id']
    assert results[0].semantic_type == 'constraint'
```

### Week 1 Success Metrics

**Quantitative Targets:**
- ✅ P1 agents use semantic search for duplicate detection
- ✅ Duplicate rate: <8% (baseline: ~15%)
- ✅ Merge precision: >80% (manually review 20 MERGE operations)
- ✅ No false negatives: Novel content not merged incorrectly

**Validation Checklist:**
- [ ] Migration applied to production database
- [ ] Existing blocks have embeddings generated (backfill script)
- [ ] P1 agent successfully proposes MERGE for duplicates
- [ ] Semantic search API returns correct results with filters
- [ ] Test suite passes (10+ test cases)

---

## Week 2: P2 Causal Relationship Inference

### Goal
P2 agents infer causal relationships between blocks, achieving 60%+ block coverage.

### P2 Agent Implementation (Day 6-8)

#### File: `api/src/services/pillar2_graph_agent.py`

```python
"""
P2 Graph Agent - V3.1 Causal Relationship Inference

Infers relationships: addresses, supports, contradicts, depends_on
"""

from typing import List, Dict, Any
from supabase import Client
from .semantic_primitives import (
    semantic_search,
    SemanticSearchFilters,
    RelationshipProposal,
    infer_relationships  # Implement this week
)
import openai
import os

# Relationship ontology
RELATIONSHIP_ONTOLOGY = {
    'addresses': {
        'from_types': ['action', 'insight', 'objective', 'solution'],
        'to_types': ['problem', 'constraint', 'issue'],
        'description': 'Solution/action addresses problem/constraint',
        'prompt_template': 'Does "{from_text}" address or solve the problem described in "{to_text}"?'
    },
    'supports': {
        'from_types': ['fact', 'finding', 'metric', 'evidence'],
        'to_types': ['objective', 'insight', 'hypothesis', 'principle'],
        'description': 'Evidence supports claim/objective',
        'prompt_template': 'Does the evidence in "{from_text}" support the claim in "{to_text}"?'
    },
    'contradicts': {
        'from_types': ['fact', 'finding', 'insight'],
        'to_types': ['assumption', 'fact', 'insight', 'principle'],
        'description': 'Conflicts with existing statement',
        'prompt_template': 'Does "{from_text}" contradict or conflict with "{to_text}"?'
    },
    'depends_on': {
        'from_types': ['action', 'objective', 'task'],
        'to_types': ['action', 'objective', 'constraint', 'principle'],
        'description': 'Prerequisite dependency',
        'prompt_template': 'Does "{from_text}" require "{to_text}" to be completed first?'
    }
}

async def verify_relationship_with_llm(
    from_block: Dict[str, Any],
    to_block: Dict[str, Any],
    relationship_type: str
) -> Dict[str, Any]:
    """
    Use LLM to verify if relationship exists between blocks.

    Returns:
        {
            'exists': bool,
            'confidence_score': float,
            'reasoning': str
        }
    """
    ontology = RELATIONSHIP_ONTOLOGY[relationship_type]
    prompt_template = ontology['prompt_template']

    verification_prompt = f"""
You are verifying a causal relationship between two blocks of knowledge.

Relationship Type: {relationship_type}
Description: {ontology['description']}

From Block (Type: {from_block['semantic_type']}):
{from_block['text_content']}

To Block (Type: {to_block['semantic_type']}):
{to_block['text_content']}

Question: {prompt_template.format(from_text=from_block['text_content'][:200], to_text=to_block['text_content'][:200])}

Respond in JSON format:
{{
    "exists": true/false,
    "confidence_score": 0.0-1.0,
    "reasoning": "Brief explanation (1-2 sentences)"
}}
"""

    openai.api_key = os.getenv("OPENAI_API_KEY")

    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": verification_prompt}],
        response_format={"type": "json_object"},
        temperature=0.1
    )

    import json
    return json.loads(response.choices[0].message.content)

async def infer_relationships_for_block(
    supabase: Client,
    block_id: str,
    basket_id: str
) -> List[RelationshipProposal]:
    """
    Infer all possible relationships for a given block.

    Process:
    1. Fetch source block
    2. For each relationship type, search for candidate target blocks
    3. Verify candidates with LLM
    4. Return high-confidence proposals
    """
    # Fetch source block
    source_block = await supabase.table('blocks').select('*').eq('id', block_id).single().execute()
    source_block = source_block.data

    proposals = []

    # Try each relationship type
    for rel_type, ontology in RELATIONSHIP_ONTOLOGY.items():
        # Check if source block type is valid for this relationship
        if source_block['semantic_type'] not in ontology['from_types']:
            continue

        # Search for candidate target blocks
        candidates = await semantic_search(
            supabase=supabase,
            basket_id=basket_id,
            query_text=source_block['text_content'],
            filters=SemanticSearchFilters(
                semantic_types=ontology['to_types'],
                states=['ACCEPTED', 'ACTIVE'],
                min_similarity=0.65  # Lower threshold for relationship discovery
            ),
            limit=10
        )

        # Verify each candidate with LLM
        for candidate in candidates[:3]:  # Limit to top 3 candidates per type
            verification = await verify_relationship_with_llm(
                from_block=source_block,
                to_block={
                    'semantic_type': candidate.semantic_type,
                    'text_content': candidate.text_content
                },
                relationship_type=rel_type
            )

            if verification['exists'] and verification['confidence_score'] >= 0.70:
                proposals.append(RelationshipProposal(
                    from_block_id=block_id,
                    to_block_id=candidate.id,
                    relationship_type=rel_type,
                    confidence_score=verification['confidence_score'],
                    inference_method='semantic_search',
                    reasoning=verification['reasoning']
                ))

    return proposals

async def execute_graph_inference_phase(
    supabase: Client,
    basket_id: str,
    block_ids: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Execute P2 graph inference for basket or specific blocks.

    Args:
        basket_id: Basket to analyze
        block_ids: Optional specific blocks (if None, analyze all ACCEPTED blocks)
    """
    # Fetch blocks to analyze
    if block_ids:
        blocks_query = supabase.table('blocks').select('id').in_('id', block_ids)
    else:
        blocks_query = supabase.table('blocks').select('id').eq('basket_id', basket_id).eq('state', 'ACCEPTED')

    blocks = blocks_query.execute()
    block_ids = [b['id'] for b in blocks.data]

    # Infer relationships for each block
    all_proposals = []
    for block_id in block_ids:
        proposals = await infer_relationships_for_block(supabase, block_id, basket_id)
        all_proposals.extend(proposals)

    # Create relationship records (with governance state)
    relationships_created = 0
    for proposal in all_proposals:
        # Auto-accept high confidence (>0.90)
        state = 'ACCEPTED' if proposal.confidence_score > 0.90 else 'PROPOSED'

        try:
            await supabase.table('substrate_relationships').insert({
                'from_block_id': proposal.from_block_id,
                'to_block_id': proposal.to_block_id,
                'relationship_type': proposal.relationship_type,
                'confidence_score': proposal.confidence_score,
                'inference_method': proposal.inference_method,
                'state': state,
                'metadata': {'reasoning': proposal.reasoning}
            }).execute()
            relationships_created += 1
        except Exception as e:
            # Handle duplicate constraint (relationship already exists)
            if 'duplicate key value' in str(e):
                continue
            raise

    return {
        'blocks_analyzed': len(block_ids),
        'relationships_proposed': len(all_proposals),
        'relationships_created': relationships_created,
        'high_confidence_auto_accepted': sum(1 for p in all_proposals if p.confidence_score > 0.90)
    }
```

#### Update `semantic_primitives.py` (Day 8)

```python
# Implement the infer_relationships stub from Week 1

async def infer_relationships(
    supabase: Client,
    block_id: str,
    basket_id: str
) -> List[RelationshipProposal]:
    """
    Infer causal relationships for a block.

    Wrapper around P2 graph agent logic for reuse by other agents.
    """
    from .pillar2_graph_agent import infer_relationships_for_block
    return await infer_relationships_for_block(supabase, block_id, basket_id)
```

### Background Job: Batch Relationship Inference (Day 9)

#### File: `api/src/jobs/batch_relationship_inference.py`

```python
"""
Background job to infer relationships for existing baskets.

Usage:
    python -m api.src.jobs.batch_relationship_inference --workspace-id <id>
"""

import asyncio
from supabase import create_client
from api.src.services.pillar2_graph_agent import execute_graph_inference_phase
import os

async def infer_relationships_for_workspace(workspace_id: str):
    """Infer relationships for all baskets in workspace"""
    supabase = create_client(
        os.getenv("SUPABASE_URL"),
        os.getenv("SUPABASE_SERVICE_KEY")
    )

    # Fetch all baskets in workspace
    baskets = supabase.table('baskets').select('id').eq('workspace_id', workspace_id).execute()

    for basket in baskets.data:
        print(f"Processing basket {basket['id']}...")
        result = await execute_graph_inference_phase(supabase, basket['id'])
        print(f"  - {result['relationships_created']} relationships created")
        await asyncio.sleep(2)  # Rate limiting

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--workspace-id", required=True)
    args = parser.parse_args()

    asyncio.run(infer_relationships_for_workspace(args.workspace_id))
```

### Testing & Validation (Day 10)

#### Test: `tests/test_p2_relationship_inference.py`

```python
import pytest
from api.src.services.pillar2_graph_agent import (
    infer_relationships_for_block,
    verify_relationship_with_llm,
    execute_graph_inference_phase
)

@pytest.mark.asyncio
async def test_addresses_relationship_inference(supabase_client, test_basket):
    """Test 'addresses' relationship inference"""

    # Setup: Create problem block
    problem_block = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="User authentication is vulnerable to brute force attacks",
        semantic_type="problem"
    )

    # Create solution block
    solution_block = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Implement rate limiting on login endpoint (max 5 attempts per minute)",
        semantic_type="action"
    )

    # Infer relationships
    proposals = await infer_relationships_for_block(
        supabase_client,
        block_id=solution_block['id'],
        basket_id=test_basket['id']
    )

    # Assert 'addresses' relationship proposed
    addresses_proposals = [p for p in proposals if p.relationship_type == 'addresses']
    assert len(addresses_proposals) >= 1
    assert addresses_proposals[0].to_block_id == problem_block['id']
    assert addresses_proposals[0].confidence_score >= 0.70

@pytest.mark.asyncio
async def test_relationship_precision_no_false_positives(supabase_client, test_basket):
    """Test that unrelated blocks don't create relationships"""

    # Setup: Create unrelated blocks
    block1 = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Database migration completed successfully on 2025-01-15",
        semantic_type="fact"
    )

    block2 = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="User prefers dark mode theme for UI",
        semantic_type="preference"
    )

    # Infer relationships
    proposals = await infer_relationships_for_block(
        supabase_client,
        block_id=block1['id'],
        basket_id=test_basket['id']
    )

    # Assert no relationships to unrelated block
    assert not any(p.to_block_id == block2['id'] for p in proposals)
```

### Week 2 Success Metrics

**Quantitative Targets:**
- ✅ P2 agents infer relationships automatically
- ✅ Relationship precision: >80% (manually review 50 relationships)
- ✅ Block coverage: >60% of blocks have at least 1 relationship
- ✅ Relationship distribution: Each type (addresses, supports, contradicts, depends_on) represented

**Validation Checklist:**
- [ ] P2 agent successfully infers relationships for new blocks
- [ ] High confidence relationships (>0.90) auto-accepted
- [ ] Relationship graph traversal queries work correctly
- [ ] Batch inference job processes existing baskets
- [ ] Test suite passes (15+ test cases)

---

## Week 3: P3/P4 Semantic Retrieval + Causal Reasoning

### Goal
P3/P4 agents use semantic search and relationship traversal for richer insights and documents.

### Complete Semantic Primitives (Day 11)

#### Update `semantic_primitives.py`

```python
# Implement traverse_relationships (stub from Week 1)

async def traverse_relationships(
    supabase: Client,
    start_block_id: str,
    relationship_type: str,
    direction: str = 'forward',
    max_depth: int = 2
) -> List[BlockWithDepth]:
    """
    Traverse relationship graph from starting block.

    Args:
        start_block_id: Starting block
        relationship_type: Type of relationship to follow
        direction: 'forward' (from -> to) or 'backward' (to -> from)
        max_depth: Maximum traversal depth

    Returns:
        List of blocks with depth level
    """
    direction_field = 'to_block_id' if direction == 'forward' else 'from_block_id'
    source_field = 'from_block_id' if direction == 'forward' else 'to_block_id'

    query = f"""
        WITH RECURSIVE relationship_chain AS (
            -- Base case: start block
            SELECT
                b.id,
                b.text_content,
                b.semantic_type,
                0 AS depth,
                r.relationship_type
            FROM public.blocks b
            LEFT JOIN public.substrate_relationships r ON b.id = r.{source_field}
            WHERE b.id = $1

            UNION ALL

            -- Recursive case: follow relationships
            SELECT
                b.id,
                b.text_content,
                b.semantic_type,
                rc.depth + 1,
                r.relationship_type
            FROM relationship_chain rc
            JOIN public.substrate_relationships r ON rc.id = r.{source_field}
            JOIN public.blocks b ON r.{direction_field} = b.id
            WHERE rc.depth < $2
                AND r.relationship_type = $3
                AND r.state = 'ACCEPTED'
        )
        SELECT DISTINCT ON (id) * FROM relationship_chain
        ORDER BY id, depth
    """

    response = supabase.rpc('execute_sql', {
        'query': query,
        'params': [start_block_id, max_depth, relationship_type]
    }).execute()

    return [
        BlockWithDepth(
            id=row['id'],
            text_content=row['text_content'],
            semantic_type=row['semantic_type'],
            depth=row['depth'],
            relationship_type=row['relationship_type']
        )
        for row in response.data
    ]
```

### P3 Agent Enhancement (Day 12-13)

#### File: `api/src/services/pillar3_insights_agent.py`

**Changes:**

```python
from .semantic_primitives import (
    semantic_search_cross_basket,
    traverse_relationships
)

async def generate_insight(
    supabase: Client,
    basket_id: str,
    insight_request: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate P3 insight with V3.1 semantic retrieval + graph traversal.
    """
    query = insight_request['query']  # e.g., "Why is authentication failing?"

    # 1. V3.1: Semantic search for relevant context (instead of full basket scan)
    relevant_blocks = await semantic_search(
        supabase=supabase,
        basket_id=basket_id,
        query_text=query,
        filters=SemanticSearchFilters(
            semantic_types=['fact', 'finding', 'constraint', 'issue'],
            states=['ACCEPTED', 'ACTIVE'],
            min_similarity=0.65
        ),
        limit=15
    )

    # 2. V3.1: For causal "why" questions, traverse relationship graph
    causal_context = []
    if 'why' in query.lower() or 'cause' in query.lower():
        for block in relevant_blocks[:3]:
            # Traverse 'addresses' relationships backward (what problems does this address?)
            problems = await traverse_relationships(
                supabase=supabase,
                start_block_id=block.id,
                relationship_type='addresses',
                direction='backward',
                max_depth=2
            )

            # Traverse 'supports' relationships (what evidence supports this?)
            evidence = await traverse_relationships(
                supabase=supabase,
                start_block_id=block.id,
                relationship_type='supports',
                direction='backward',
                max_depth=1
            )

            causal_context.extend(problems)
            causal_context.extend(evidence)

    # 3. Build context for LLM with semantic + causal information
    context_blocks = relevant_blocks + causal_context
    context_text = "\n\n".join([
        f"[{b.semantic_type}] {b.text_content}"
        for b in context_blocks
    ])

    # 4. Generate insight (existing LLM logic)
    insight_text = await generate_insight_with_llm(query, context_text)

    # 5. Create P3 insight block (existing logic)
    insight_block = await supabase.table('blocks').insert({
        'basket_id': basket_id,
        'semantic_type': 'insight',
        'anchor_role': 'P3_INSIGHT',
        'text_content': insight_text,
        'state': 'ACTIVE',
        'metadata': {
            'context_blocks_used': [b.id for b in context_blocks],
            'semantic_search_enabled': True,
            'causal_reasoning_enabled': len(causal_context) > 0
        }
    }).execute()

    return {
        'insight_id': insight_block.data[0]['id'],
        'insight_text': insight_text,
        'context_blocks_used': len(context_blocks),
        'causal_reasoning_applied': len(causal_context) > 0
    }
```

### P4 Agent Enhancement (Day 13-14)

#### File: `api/src/services/pillar4_composition_agent.py`

**Changes:**

```python
from .semantic_primitives import (
    semantic_search,
    traverse_relationships
)

async def generate_document(
    supabase: Client,
    basket_id: str,
    theme: str
) -> Dict[str, Any]:
    """
    Generate P4 document with V3.1 semantic theme retrieval + causal flow.
    """
    # 1. V3.1: Semantic search for theme-relevant blocks
    theme_blocks = await semantic_search(
        supabase=supabase,
        basket_id=basket_id,
        query_text=theme,
        filters=SemanticSearchFilters(
            anchor_roles=['ACCEPTED', 'P3_INSIGHT'],  # Include substrate + insights
            min_similarity=0.60
        ),
        limit=30
    )

    # 2. V3.1: Follow relationship chains for narrative structure
    narrative_structure = []

    # Find "problem" blocks
    problems = [b for b in theme_blocks if b.semantic_type in ['problem', 'issue', 'constraint']]

    for problem in problems[:5]:  # Top 5 problems
        # Traverse 'addresses' to find solutions
        solutions = await traverse_relationships(
            supabase=supabase,
            start_block_id=problem.id,
            relationship_type='addresses',
            direction='backward',
            max_depth=1
        )

        # For each solution, find supporting evidence
        for solution in solutions:
            evidence = await traverse_relationships(
                supabase=supabase,
                start_block_id=solution.id,
                relationship_type='supports',
                direction='backward',
                max_depth=1
            )

            narrative_structure.append({
                'problem': problem,
                'solution': solution,
                'evidence': evidence
            })

    # 3. Compose document with causal narrative flow (existing LLM logic)
    document_text = await compose_document_with_llm(
        theme=theme,
        narrative_structure=narrative_structure,
        additional_blocks=theme_blocks
    )

    # 4. Create P4 document block (existing logic)
    document_block = await supabase.table('blocks').insert({
        'basket_id': basket_id,
        'semantic_type': 'document',
        'anchor_role': 'P4_DOCUMENT',
        'text_content': document_text,
        'state': 'ACTIVE',
        'metadata': {
            'theme': theme,
            'blocks_used': [b.id for b in theme_blocks],
            'narrative_chains': len(narrative_structure),
            'semantic_composition_enabled': True
        }
    }).execute()

    return {
        'document_id': document_block.data[0]['id'],
        'document_text': document_text,
        'narrative_chains_used': len(narrative_structure)
    }
```

### Testing & Validation (Day 14-15)

#### Test: `tests/test_p3_semantic_insights.py`

```python
import pytest
from api.src.services.pillar3_insights_agent import generate_insight

@pytest.mark.asyncio
async def test_p3_causal_reasoning_why_question(supabase_client, test_basket):
    """Test P3 uses graph traversal for 'why' questions"""

    # Setup: Create problem -> solution -> evidence chain
    problem = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Login failures increased 40% last week",
        semantic_type="problem"
    )

    solution = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Added rate limiting to prevent brute force",
        semantic_type="action"
    )

    evidence = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Login failures dropped to 5% after rate limiting deployed",
        semantic_type="fact"
    )

    # Create relationships
    await create_relationship(supabase_client, solution['id'], problem['id'], 'addresses')
    await create_relationship(supabase_client, evidence['id'], solution['id'], 'supports')

    # Generate insight with "why" question
    result = await generate_insight(
        supabase_client,
        basket_id=test_basket['id'],
        insight_request={'query': 'Why did login failures decrease?'}
    )

    # Assert causal reasoning applied
    assert result['causal_reasoning_applied'] is True

    # Assert insight mentions relationship chain
    insight_text = result['insight_text']
    assert 'rate limiting' in insight_text.lower()
    assert 'brute force' in insight_text.lower()
```

#### Test: `tests/test_p4_semantic_composition.py`

```python
import pytest
from api.src.services.pillar4_composition_agent import generate_document

@pytest.mark.asyncio
async def test_p4_narrative_structure_with_relationships(supabase_client, test_basket):
    """Test P4 follows relationship chains for narrative"""

    # Setup: Create problem-solution-evidence narrative
    problem = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="User onboarding completion rate is only 40%",
        semantic_type="problem"
    )

    solution = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Simplified onboarding to 3 steps instead of 7",
        semantic_type="action"
    )

    evidence = await create_block_with_embedding(
        supabase_client,
        basket_id=test_basket['id'],
        text_content="Onboarding completion increased to 75% after simplification",
        semantic_type="metric"
    )

    # Create relationships
    await create_relationship(supabase_client, solution['id'], problem['id'], 'addresses')
    await create_relationship(supabase_client, evidence['id'], solution['id'], 'supports')

    # Generate document with theme
    result = await generate_document(
        supabase_client,
        basket_id=test_basket['id'],
        theme="User onboarding improvements"
    )

    # Assert narrative chains used
    assert result['narrative_chains_used'] >= 1

    # Assert document follows problem -> solution -> evidence flow
    document_text = result['document_text']
    problem_pos = document_text.find('40%')
    solution_pos = document_text.find('Simplified')
    evidence_pos = document_text.find('75%')

    assert problem_pos < solution_pos < evidence_pos  # Correct narrative order
```

### Week 3 Success Metrics

**Quantitative Targets:**
- ✅ P3 agents use semantic search (not full basket scan)
- ✅ P3 causal reasoning: 80%+ of "why" questions traverse graph
- ✅ P4 narrative coherence: 90%+ narrative chains follow causal flow
- ✅ Context relevance: 85%+ of retrieved blocks semantically relevant

**Validation Checklist:**
- [ ] P3 insights use semantic search for context retrieval
- [ ] P3 "why" questions trigger graph traversal
- [ ] P4 documents follow relationship chains for narrative
- [ ] Cross-basket semantic search works (P3 scoped insights)
- [ ] Test suite passes (20+ test cases)

---

## Final Integration & Deployment (Day 16-21)

### Frontend Integration (Day 16-17)

#### API Endpoints

**New endpoint**: `GET /api/baskets/:id/semantic-search`

```typescript
// Query: ?q=login+rate+limiting&types=constraint,action&limit=10
interface SemanticSearchRequest {
  q: string;
  types?: string[];
  limit?: number;
}

interface SemanticSearchResponse {
  results: Array<{
    block_id: string;
    text_content: string;
    semantic_type: string;
    similarity_score: number;
  }>;
}
```

**New endpoint**: `GET /api/blocks/:id/relationships`

```typescript
interface BlockRelationshipsResponse {
  block_id: string;
  relationships: Array<{
    id: string;
    relationship_type: 'addresses' | 'supports' | 'contradicts' | 'depends_on';
    direction: 'outgoing' | 'incoming';
    related_block: {
      id: string;
      text_content: string;
      semantic_type: string;
    };
    confidence_score: number;
  }>;
}
```

#### UI Components

**Component**: `components/SemanticSearch.tsx`

```typescript
// Semantic search bar with type filters
// Shows similarity scores in results
// Clicking result navigates to block detail
```

**Component**: `components/BlockRelationshipGraph.tsx`

```typescript
// Visual graph showing block relationships
// Interactive: click node to navigate
// Color-coded by relationship type
```

### Performance Optimization (Day 18)

1. **Embedding generation**: Move to async background job (Celery/BullMQ)
2. **Semantic search caching**: Cache query embeddings for 5 minutes
3. **Relationship inference batching**: Process max 50 blocks per job
4. **Database indexing**: Verify IVFFlat index performance (rebuild if needed)

### Monitoring & Observability (Day 19)

#### Metrics to Track

```python
# Add to existing metrics endpoint
{
    "semantic_layer": {
        "embeddings": {
            "total_blocks": 1500,
            "blocks_with_embeddings": 1450,
            "coverage_percentage": 96.7
        },
        "relationships": {
            "total_relationships": 890,
            "by_type": {
                "addresses": 320,
                "supports": 410,
                "contradicts": 80,
                "depends_on": 80
            },
            "blocks_with_relationships": 950,
            "coverage_percentage": 63.3
        },
        "semantic_search": {
            "avg_query_time_ms": 45,
            "cache_hit_rate": 0.72
        }
    }
}
```

### Documentation (Day 20)

#### Update User Documentation

- **Guide**: "Using Semantic Search" (with examples)
- **Guide**: "Understanding Block Relationships" (relationship types explained)
- **Changelog**: v3.1 release notes

#### Update Developer Documentation

- **API Reference**: Semantic primitives API
- **Architecture**: Semantic layer integration diagrams
- **Migration Guide**: v3.0 → v3.1 upgrade steps

### Final Validation & Launch (Day 21)

**Pre-launch Checklist:**
- [ ] All 3 weeks' success metrics met
- [ ] Test suite: 50+ tests passing
- [ ] Performance: Semantic search <100ms p95
- [ ] Manual QA: Test all pillar integrations end-to-end
- [ ] Documentation: User + developer guides complete
- [ ] Monitoring: Metrics dashboards configured
- [ ] Rollback plan: Database migration rollback tested

**Launch Sequence:**
1. Deploy to staging → Run full test suite
2. Deploy to production (off-hours)
3. Monitor metrics for 24 hours
4. Announce v3.1 release to users

---

## Cost Model & Resource Requirements

### OpenAI API Costs

| Operation | Model | Cost per 1K | Volume | Monthly Cost |
|-----------|-------|-------------|--------|--------------|
| Embedding generation | text-embedding-3-small | $0.00002 | 10K blocks/month | $0.20 |
| Relationship verification | gpt-4o-mini | $0.00015 (input) + $0.0006 (output) | 5K verifications/month | $3.75 |
| **Total** | | | | **~$4/month** |

At scale (100K blocks): ~$40/month

### Infrastructure Requirements

- **PostgreSQL**: pgvector extension (already available in Supabase)
- **Compute**: No additional servers (async jobs use existing workers)
- **Storage**: +6KB per block (1536-dim vector = 6KB)
  - 10K blocks = 60MB additional storage
  - Negligible cost

### Developer Time

- **Week 1**: 5 days (infrastructure + P1)
- **Week 2**: 5 days (P2 graph inference)
- **Week 3**: 5 days (P3/P4 enhancements)
- **Integration**: 6 days (frontend, docs, deployment)
- **Total**: 21 days (~4 weeks with buffer)

---

## Risk Mitigation

### Technical Risks

| Risk | Impact | Mitigation |
|------|--------|------------|
| Embedding generation slow | P1 latency increases | Async background jobs (don't block evolution) |
| Relationship precision low (<80%) | Noisy graph | LLM verification + confidence thresholds |
| Semantic search irrelevant results | Poor P3/P4 quality | Hybrid filters (type + anchor_role + similarity) |
| pgvector index performance | Query latency | IVFFlat tuning + caching |

### Product Risks

| Risk | Impact | Mitigation |
|------|--------|------------|
| Users don't understand relationships | Low adoption | In-app guidance + examples |
| Semantic search doesn't match expectations | User confusion | Clear similarity score display + fallback to keyword search |
| Cost scales unexpectedly | Budget overrun | Per-workspace rate limiting + usage monitoring |

---

## Post-Launch Optimization (Week 4+)

### Iteration 1: Quality Refinement
- Review relationship precision manually (sample 100 relationships)
- Tune confidence score thresholds based on false positives/negatives
- Add user feedback loop ("Is this relationship correct?")

### Iteration 2: Performance Optimization
- Implement query result caching (Redis)
- Optimize embedding generation (batch API calls)
- Pre-compute frequent semantic searches

### Iteration 3: Feature Expansion
- User-created relationships (override agent inference)
- Relationship annotations ("why does this contradict?")
- Multi-hop graph queries in UI ("Show me all evidence supporting this objective")

---

## Success Criteria Summary

**Week 1 (P1 Semantic Duplicate Detection):**
- ✅ Duplicate rate: <8% (baseline: ~15%)
- ✅ Merge precision: >80%

**Week 2 (P2 Relationship Inference):**
- ✅ Relationship precision: >80%
- ✅ Block coverage: >60% have relationships

**Week 3 (P3/P4 Semantic Retrieval):**
- ✅ P3 causal reasoning: 80%+ "why" questions use graph
- ✅ P4 narrative coherence: 90%+ follow causal flow

**Overall (v3.1 Launch):**
- ✅ All pillar agents use semantic layer
- ✅ Test suite: 50+ tests passing
- ✅ Performance: <100ms p95 semantic search
- ✅ Cost: <$10/month at 10K blocks
- ✅ Documentation complete (user + developer)

---

## Appendix: Example User Workflows

### Workflow 1: P1 Prevents Duplicate Block

**Before v3.1:**
1. User dumps "Login needs rate limiting" → CREATE new block
2. User dumps "Rate limit login endpoint" → CREATE duplicate block
3. Result: 2 duplicate blocks in substrate

**After v3.1:**
1. User dumps "Login needs rate limiting" → CREATE block A
2. User dumps "Rate limit login endpoint" → P1 semantic search finds block A (similarity 0.92)
3. P1 proposes MERGE → User approves → No duplicate
4. Result: 1 enriched block

### Workflow 2: P3 Answers "Why" Question with Causal Reasoning

**User asks:** "Why did API latency decrease?"

**Before v3.1:**
- P3 scans all blocks → Finds "API latency decreased"
- Generic insight: "Latency improved based on metrics"

**After v3.1:**
1. P3 semantic search finds "API latency decreased 40%"
2. P3 traverses 'addresses' backward → Finds "Added caching layer"
3. P3 traverses 'supports' backward → Finds "Cache hit rate 85%"
4. Insight: "API latency decreased due to caching implementation. Cache hit rate of 85% reduced database queries, resulting in 40% latency improvement."

### Workflow 3: P4 Composes Narrative Document

**User requests:** "Document our authentication improvements"

**Before v3.1:**
- P4 filters blocks by keyword "auth" → Random order
- Generic document: Bullet points of auth-related facts

**After v3.1:**
1. P4 semantic search: "authentication improvements" → Finds 30 relevant blocks
2. P4 identifies problem: "Brute force vulnerability"
3. P4 follows 'addresses' → Solution: "Rate limiting"
4. P4 follows 'supports' → Evidence: "Attack attempts dropped 95%"
5. Document: Narrative flow from problem → solution → outcome

---

**End of Implementation Sequencing Plan**
