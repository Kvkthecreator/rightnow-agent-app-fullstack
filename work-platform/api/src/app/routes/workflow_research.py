"""
Deterministic Research Workflow Endpoint

Part of Workflow-First Architecture (Phase 1):
- Explicit parameters (no TP orchestration)
- Direct specialist invocation
- Full context loading (WorkBundle pattern)
- Auditable execution tracking

Strategic Goal: Prove specialist agents work reliably before adding
TP intelligence layer (Phase 2)
"""

from typing import Optional
from uuid import UUID
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel

from app.utils.jwt import verify_jwt
from app.utils.supabase_client import supabase_admin_client as supabase
from agents_sdk.research_agent_sdk import ResearchAgentSDK
from agents_sdk.work_bundle import WorkBundle
from yarnnn_agents.session import AgentSession
import logging

router = APIRouter(prefix="/work/research", tags=["workflows"])
logger = logging.getLogger(__name__)


class ResearchWorkflowRequest(BaseModel):
    """Deterministic research workflow parameters."""
    basket_id: str
    task_description: str
    research_scope: Optional[str] = "general"  # general, competitor, market, technical
    depth: Optional[str] = "standard"  # quick, standard, deep
    output_format: Optional[str] = "markdown"  # markdown, json, structured
    priority: Optional[int] = 5


class ResearchWorkflowResponse(BaseModel):
    """Research workflow execution result."""
    work_request_id: str
    work_ticket_id: str
    agent_session_id: str
    status: str  # pending, running, completed, failed
    outputs: list[dict]  # work_outputs generated by agent
    execution_time_ms: Optional[int]
    message: str


@router.post("/execute", response_model=ResearchWorkflowResponse)
async def execute_research_workflow(
    request: ResearchWorkflowRequest,
    user: dict = Depends(verify_jwt)
):
    """
    Execute deterministic research workflow.

    Flow:
    1. Validate permissions (workspace, basket, trial limits)
    2. Load context (WorkBundle: blocks + assets + config)
    3. Create work_request + work_ticket (tracking)
    4. Execute ResearchAgentSDK with context
    5. Return structured outputs

    Args:
        request: Research workflow parameters
        user: Authenticated user from JWT

    Returns:
        Research workflow execution result with outputs

    Raises:
        401: Authentication failed
        403: Permission denied or trial limit exceeded
        404: Basket not found
        500: Execution error
    """
    user_id = user.get("sub") or user.get("user_id")
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid user token")

    logger.info(
        f"[RESEARCH WORKFLOW] Starting: user={user_id}, basket={request.basket_id}"
    )

    try:
        # Step 1: Validate basket access and get workspace
        basket_response = supabase.table("baskets").select(
            "id, workspace_id, name"
        ).eq("id", request.basket_id).single().execute()

        if not basket_response.data:
            raise HTTPException(status_code=404, detail="Basket not found")

        basket = basket_response.data
        workspace_id = basket["workspace_id"]

        # TODO: Add workspace permission check (verify user has access)
        # TODO: Add trial limit check (work_requests count)

        # Step 2: Get or create research agent session (persistent per basket)
        research_session = await AgentSession.get_or_create(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            agent_type="research",
            user_id=user_id,
        )

        logger.info(
            f"[RESEARCH WORKFLOW] Agent session: {research_session.id}"
        )

        # Step 3: Create work_request (for trial tracking & billing)
        work_request_data = {
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "requested_by_user_id": user_id,
            "request_type": "research_workflow",
            "task_intent": request.task_description,
            "parameters": {
                "research_scope": request.research_scope,
                "depth": request.depth,
                "output_format": request.output_format,
            },
            "priority": "normal",
        }
        work_request_response = supabase.table("work_requests").insert(
            work_request_data
        ).execute()
        work_request_id = work_request_response.data[0]["id"]

        # Step 4: Create work_ticket (execution tracking)
        work_ticket_data = {
            "work_request_id": work_request_id,
            "agent_session_id": research_session.id,
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "agent_type": "research",
            "status": "pending",
            "metadata": {
                "workflow": "deterministic_research",
                "task_description": request.task_description,
                "research_scope": request.research_scope,
                "depth": request.depth,
            },
        }
        work_ticket_response = supabase.table("work_tickets").insert(
            work_ticket_data
        ).execute()
        work_ticket_id = work_ticket_response.data[0]["id"]

        logger.info(
            f"[RESEARCH WORKFLOW] Created: work_request={work_request_id}, "
            f"work_ticket={work_ticket_id}"
        )

        # Step 5: Load context (WorkBundle pattern)
        logger.info(f"[RESEARCH WORKFLOW] Loading context for basket {request.basket_id}")

        # 5a. Load substrate blocks (approved knowledge)
        blocks_response = supabase.table("blocks").select(
            "id, content, semantic_type, state, created_at, metadata"
        ).eq("basket_id", request.basket_id).in_(
            "state", ["ACCEPTED", "LOCKED", "CONSTANT"]  # Only approved blocks
        ).order("created_at", ascending=False).limit(200).execute()

        substrate_blocks = blocks_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(substrate_blocks)} substrate blocks")

        # 5b. Load prior work outputs (recent research to avoid duplication)
        prior_outputs_response = supabase.table("work_outputs").select(
            "id, title, output_type, body, confidence, created_at"
        ).eq("basket_id", request.basket_id).eq(
            "agent_type", "research"
        ).eq("status", "approved").order(
            "created_at", ascending=False
        ).limit(50).execute()

        prior_work_outputs = prior_outputs_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(prior_work_outputs)} prior outputs")

        # 5c. Load reference assets (documents, screenshots, etc.)
        assets_response = supabase.table("documents").select(
            "id, name, asset_type, url, metadata"
        ).eq("basket_id", request.basket_id).execute()

        reference_assets = assets_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(reference_assets)} reference assets")

        # 5d. Load agent config (if exists)
        config_response = supabase.table("agent_configs").select(
            "config"
        ).eq("agent_type", "research").eq(
            "workspace_id", workspace_id
        ).maybeSingle().execute()

        agent_config = config_response.data.get("config", {}) if config_response.data else {}

        # 5e. Create WorkBundle with loaded context
        context_bundle = WorkBundle(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            user_id=user_id,
            task=request.task_description,
            agent_type="research",
            priority="medium",
            substrate_blocks=substrate_blocks,
            reference_assets=reference_assets,
            agent_config=agent_config,
        )

        logger.info(
            f"[RESEARCH WORKFLOW] WorkBundle created: {len(substrate_blocks)} blocks, "
            f"{len(reference_assets)} assets, {len(prior_work_outputs)} prior outputs"
        )

        # Step 6: Update work_ticket status to running
        supabase.table("work_tickets").update({
            "status": "running",
            "started_at": "now()",
        }).eq("id", work_ticket_id).execute()

        # Step 7: Execute ResearchAgentSDK with context
        logger.info(f"[RESEARCH WORKFLOW] Executing ResearchAgentSDK")

        # Initialize ResearchAgentSDK with bundle
        research_sdk = ResearchAgentSDK(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            work_ticket_id=work_ticket_id,
            session=research_session,
            bundle=context_bundle,  # Pass WorkBundle with loaded context
        )

        # Build enhanced prompt with prior work context
        enhanced_task = request.task_description
        if prior_work_outputs:
            enhanced_task += "\n\n**Prior Research** (avoid duplication):\n"
            for output in prior_work_outputs[:5]:  # Show last 5
                enhanced_task += f"- {output['title']} ({output['output_type']})\n"

        # Execute deep dive research
        import time
        start_time = time.time()

        result = await research_sdk.deep_dive(
            topic=enhanced_task,
            claude_session_id=research_session.claude_session_id,
        )

        execution_time_ms = int((time.time() - start_time) * 1000)

        # Step 8: Update work_ticket status to completed
        supabase.table("work_tickets").update({
            "status": "completed",
            "completed_at": "now()",
            "metadata": {
                "execution_time_ms": execution_time_ms,
                "output_count": result["output_count"],
                "claude_session_id": result.get("claude_session_id"),
            },
        }).eq("id", work_ticket_id).execute()

        logger.info(
            f"[RESEARCH WORKFLOW] Execution complete: {result['output_count']} outputs "
            f"in {execution_time_ms}ms"
        )

        return ResearchWorkflowResponse(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            agent_session_id=research_session.id,
            status="completed",
            outputs=result["work_outputs"],
            execution_time_ms=execution_time_ms,
            message=f"Research complete: {result['output_count']} outputs generated",
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"[RESEARCH WORKFLOW] Failed: {e}")

        # Update work_ticket to failed status if it exists
        if 'work_ticket_id' in locals():
            try:
                supabase.table("work_tickets").update({
                    "status": "failed",
                    "completed_at": "now()",
                    "metadata": {
                        "error": str(e),
                        "error_type": type(e).__name__,
                    },
                }).eq("id", work_ticket_id).execute()
            except Exception as update_error:
                logger.error(f"Failed to update work_ticket status: {update_error}")

        raise HTTPException(
            status_code=500,
            detail=f"Research workflow execution failed: {str(e)}"
        )
