"""
V3.1 Semantic Layer Shared Primitives

Provides semantic search and relationship inference capabilities for P1-P4 agents.
This is the shared API layer that all agents use for semantic operations.

Design Principles:
1. Embed substrate, not raw input (ACCEPTED+ blocks only)
2. Agent-first, not user-first (primary value is agent intelligence)
3. Structure + semantics (hybrid search with filters)
4. Deliberate integration (clean contracts for P1-P4)

Reference: docs/SEMANTIC_LAYER_INTEGRATION_DESIGN.md
Canon: docs/YARNNN_CANON.md v3.1
"""

import logging
import os
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from uuid import UUID

from supabase import Client
from openai import OpenAI

logger = logging.getLogger("uvicorn.error")

# ============================================================================
# Configuration
# ============================================================================

# V3.1 uses text-embedding-3-small (1536 dimensions) for cost-efficiency
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536

# Confidence thresholds for P1 duplicate detection
DUPLICATE_HIGH_CONFIDENCE = 0.85  # MERGE operation
DUPLICATE_MEDIUM_CONFIDENCE = 0.70  # UPDATE/ENRICH operation

# Confidence thresholds for P2 relationship inference
RELATIONSHIP_HIGH_CONFIDENCE = 0.90  # Auto-accept
RELATIONSHIP_MEDIUM_CONFIDENCE = 0.70  # Propose for review

# ============================================================================
# Data Models
# ============================================================================

@dataclass
class SemanticSearchFilters:
    """
    Filters for hybrid semantic search.
    Combines vector similarity with structured constraints.
    """
    semantic_types: Optional[List[str]] = None  # e.g., ['fact', 'constraint']
    anchor_roles: Optional[List[str]] = None    # e.g., ['problem', 'solution']
    states: Optional[List[str]] = None          # e.g., ['ACCEPTED', 'LOCKED']
    min_similarity: float = 0.70                # Minimum cosine similarity


@dataclass
class BlockWithSimilarity:
    """
    Block with semantic similarity score.
    Returned by semantic_search functions.
    """
    id: str
    basket_id: str
    content: str
    semantic_type: str
    anchor_role: Optional[str]
    state: str
    metadata: Dict[str, Any]
    similarity_score: float


@dataclass
class RelationshipProposal:
    """
    Proposed causal relationship between blocks.
    Generated by P2 graph agent using semantic search + LLM verification.
    """
    from_block_id: str
    to_block_id: str
    relationship_type: str  # addresses, supports, contradicts, depends_on
    confidence_score: float
    inference_method: str   # semantic_search, llm_verification
    reasoning: str          # LLM explanation


@dataclass
class BlockWithDepth:
    """
    Block with graph traversal depth.
    Returned by traverse_relationships for causal reasoning.
    """
    id: str
    content: str
    semantic_type: str
    anchor_role: Optional[str]
    depth: int
    relationship_type: str


# ============================================================================
# Core Primitives: Embedding Generation
# ============================================================================

def generate_embedding(text: str) -> Optional[List[float]]:
    """
    Generate vector embedding for text using OpenAI API.

    Args:
        text: Text content to embed (title + content typically)

    Returns:
        1536-dimensional embedding vector, or None if error

    Notes:
        - Uses text-embedding-3-small for cost-efficiency ($0.02/1M tokens)
        - Truncates to 8000 chars (~8K tokens max for OpenAI)
        - Errors are logged and returned as None (graceful degradation)
    """
    if not text or not text.strip():
        logger.warning("generate_embedding: Empty text provided")
        return None

    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY not set")

        client = OpenAI(api_key=api_key)

        # Truncate to prevent token limit errors
        trimmed = text[:8000]

        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=trimmed
        )

        embedding = response.data[0].embedding

        # Verify dimension size
        if len(embedding) != EMBEDDING_DIMENSIONS:
            logger.error(
                f"Unexpected embedding dimension: {len(embedding)} (expected {EMBEDDING_DIMENSIONS})"
            )
            return None

        return embedding

    except Exception as exc:
        logger.error(f"generate_embedding failed: {exc}")
        return None


# ============================================================================
# Core Primitives: Semantic Search
# ============================================================================

async def semantic_search(
    supabase: Client,
    basket_id: str,
    query_text: str,
    filters: SemanticSearchFilters,
    limit: int = 20
) -> List[BlockWithSimilarity]:
    """
    Hybrid semantic search within a basket.

    Combines vector similarity with structured filters (type, role, state).
    This is the primary search mechanism for P1 (duplicate detection) and
    P3/P4 (context retrieval).

    Args:
        supabase: Supabase client (service role)
        basket_id: Basket to search within
        query_text: Text to find similar blocks for
        filters: Structured filters (semantic_type, anchor_role, state)
        limit: Maximum results to return

    Returns:
        List of blocks with similarity scores, sorted by relevance (highest first)

    Usage (P1 duplicate detection):
        similar = await semantic_search(
            supabase, basket_id, "Rate limit login endpoint",
            SemanticSearchFilters(
                semantic_types=['constraint', 'action'],
                states=['ACCEPTED', 'LOCKED'],
                min_similarity=0.70
            )
        )
        if similar and similar[0].similarity_score > 0.85:
            # High confidence duplicate → MERGE

    Usage (P3 context retrieval):
        context = await semantic_search(
            supabase, basket_id, "Why did login failures increase?",
            SemanticSearchFilters(
                semantic_types=['fact', 'finding', 'issue'],
                min_similarity=0.65
            )
        )
    """
    try:
        # Generate query embedding
        query_embedding = generate_embedding(query_text)
        if not query_embedding:
            logger.error("semantic_search: Failed to generate query embedding")
            return []

        # Call database function (hybrid search with filters)
        response = supabase.rpc(
            'semantic_search_blocks',
            {
                'p_basket_id': str(basket_id),
                'p_query_embedding': query_embedding,
                'p_semantic_types': filters.semantic_types,
                'p_anchor_roles': filters.anchor_roles,
                'p_states': filters.states or ['ACCEPTED', 'LOCKED', 'CONSTANT'],
                'p_min_similarity': filters.min_similarity,
                'p_limit': limit
            }
        ).execute()

        if not response.data:
            return []

        # Convert to BlockWithSimilarity objects
        results = []
        for row in response.data:
            results.append(BlockWithSimilarity(
                id=row['id'],
                basket_id=row['basket_id'],
                content=row['content'],
                semantic_type=row['semantic_type'],
                anchor_role=row.get('anchor_role'),
                state=row['state'],
                metadata=row.get('metadata', {}),
                similarity_score=float(row['similarity_score'])
            ))

        return results

    except Exception as exc:
        logger.error(f"semantic_search failed: {exc}")
        return []


async def semantic_search_cross_basket(
    supabase: Client,
    workspace_id: str,
    query_text: str,
    scopes: List[str] = ['WORKSPACE', 'GLOBAL'],
    semantic_types: Optional[List[str]] = None,
    limit: int = 10
) -> List[BlockWithSimilarity]:
    """
    Cross-basket semantic search for scope elevation detection.

    Searches WORKSPACE/GLOBAL scoped blocks across all baskets in workspace.
    Used for pattern discovery and knowledge reuse.

    Args:
        supabase: Supabase client (service role)
        workspace_id: Workspace to search within
        query_text: Text to find similar blocks for
        scopes: Scope levels to include (WORKSPACE, GLOBAL)
        semantic_types: Optional semantic type filter
        limit: Maximum results to return

    Returns:
        List of blocks from other baskets with similarity scores

    Usage (P1 scope elevation):
        workspace_patterns = await semantic_search_cross_basket(
            supabase, workspace_id, "Authentication constraints",
            scopes=['WORKSPACE'],
            semantic_types=['constraint', 'principle']
        )
        # Suggest promoting this block to WORKSPACE scope
    """
    try:
        # Generate query embedding
        query_embedding = generate_embedding(query_text)
        if not query_embedding:
            logger.error("semantic_search_cross_basket: Failed to generate query embedding")
            return []

        # Call database function
        response = supabase.rpc(
            'semantic_search_cross_basket',
            {
                'p_workspace_id': str(workspace_id),
                'p_query_embedding': query_embedding,
                'p_scopes': scopes,
                'p_semantic_types': semantic_types,
                'p_min_similarity': 0.70,
                'p_limit': limit
            }
        ).execute()

        if not response.data:
            return []

        # Convert to BlockWithSimilarity objects
        results = []
        for row in response.data:
            results.append(BlockWithSimilarity(
                id=row['id'],
                basket_id=row['basket_id'],
                content=row['content'],
                semantic_type=row['semantic_type'],
                anchor_role=row.get('anchor_role'),
                state=row.get('scope', 'LOCAL'),  # Use scope as state for cross-basket
                metadata={'scope': row.get('scope')},
                similarity_score=float(row['similarity_score'])
            ))

        return results

    except Exception as exc:
        logger.error(f"semantic_search_cross_basket failed: {exc}")
        return []


# ============================================================================
# Core Primitives: Relationship Traversal (Week 2+)
# ============================================================================

async def traverse_relationships(
    supabase: Client,
    start_block_id: str,
    relationship_type: str,
    direction: str = 'forward',
    max_depth: int = 2
) -> List[BlockWithDepth]:
    """
    Recursive graph traversal following causal relationships.

    Follows a specific relationship type (addresses, supports, contradicts, depends_on)
    from a starting block to discover causal chains.

    Args:
        supabase: Supabase client (service role)
        start_block_id: Starting block for traversal
        relationship_type: Type of relationship to follow
        direction: 'forward' (from→to) or 'backward' (to→from)
        max_depth: Maximum traversal depth

    Returns:
        List of blocks with depth level, ordered by depth

    Usage (P3 causal reasoning for "why" questions):
        # User asks: "Why did API latency decrease?"
        # P3 finds block: "API latency decreased 40%"

        # Traverse backward to find what addressed this
        solutions = await traverse_relationships(
            supabase, block_id, 'addresses', direction='backward', max_depth=1
        )
        # Returns: "Added caching layer"

        # Then find evidence supporting the solution
        evidence = await traverse_relationships(
            supabase, solution_id, 'supports', direction='backward', max_depth=1
        )
        # Returns: "Cache hit rate 85%"

    Usage (P4 narrative composition):
        # Find all solutions that address this problem
        solutions = await traverse_relationships(
            supabase, problem_id, 'addresses', direction='backward', max_depth=1
        )
        # Compose narrative: Problem → Solution → Outcome
    """
    try:
        # Call database function (recursive CTE)
        response = supabase.rpc(
            'traverse_relationships',
            {
                'p_start_block_id': str(start_block_id),
                'p_relationship_type': relationship_type,
                'p_direction': direction,
                'p_max_depth': max_depth
            }
        ).execute()

        if not response.data:
            return []

        # Convert to BlockWithDepth objects
        results = []
        for row in response.data:
            results.append(BlockWithDepth(
                id=row['id'],
                content=row['content'],
                semantic_type=row['semantic_type'],
                anchor_role=row.get('anchor_role'),
                depth=row['depth'],
                relationship_type=row['relationship_type']
            ))

        return results

    except Exception as exc:
        logger.error(f"traverse_relationships failed: {exc}")
        return []


# ============================================================================
# P2 Relationship Inference (Stub for Week 2)
# ============================================================================

async def infer_relationships(
    supabase: Client,
    block_id: str,
    basket_id: str
) -> List[RelationshipProposal]:
    """
    Infer causal relationships for a block.

    This is implemented in Week 2 as part of P2 graph agent upgrade.
    Process:
    1. Semantic search to find relationship candidates
    2. LLM verification to confirm causal relationship
    3. Return high-confidence proposals

    Args:
        supabase: Supabase client (service role)
        block_id: Block to infer relationships for
        basket_id: Basket containing the block

    Returns:
        List of relationship proposals with confidence scores

    Week 2 Implementation:
        - P2 graph agent calls this for new ACCEPTED blocks
        - High confidence (>0.90) proposals auto-accepted
        - Medium confidence (0.70-0.90) proposed for user review
    """
    # TODO: Week 2 implementation
    logger.info(f"infer_relationships stub called for block {block_id}")
    return []


# ============================================================================
# Helper: Generate Embedding for Block (Background Job)
# ============================================================================

async def generate_and_store_embedding(
    supabase: Client,
    block_id: str
) -> bool:
    """
    Generate and store embedding for a block.

    Called by background job after block state transitions to ACCEPTED.
    This is async to avoid blocking P1 agent operations.

    Args:
        supabase: Supabase client (service role)
        block_id: Block to generate embedding for

    Returns:
        True if successful, False otherwise

    Usage (background job):
        # Triggered after governance approval
        if block.state == 'ACCEPTED' and not block.embedding:
            await generate_and_store_embedding(supabase, block.id)
    """
    try:
        # Fetch block
        response = supabase.table('blocks').select('id', 'title', 'content').eq('id', str(block_id)).single().execute()

        if not response.data:
            logger.error(f"Block {block_id} not found")
            return False

        block = response.data

        # Generate embedding (title + content for richer semantic representation)
        text = f"{block.get('title', '')} {block.get('content', '')}".strip()
        embedding = generate_embedding(text)

        if not embedding:
            logger.error(f"Failed to generate embedding for block {block_id}")
            return False

        # Store embedding
        supabase.table('blocks').update({
            'embedding': embedding
        }).eq('id', str(block_id)).execute()

        logger.info(f"Generated embedding for block {block_id}")
        return True

    except Exception as exc:
        logger.error(f"generate_and_store_embedding failed for block {block_id}: {exc}")
        return False


# ============================================================================
# Exports
# ============================================================================

__all__ = [
    # Data models
    'SemanticSearchFilters',
    'BlockWithSimilarity',
    'RelationshipProposal',
    'BlockWithDepth',
    # Core primitives
    'generate_embedding',
    'semantic_search',
    'semantic_search_cross_basket',
    'traverse_relationships',
    'infer_relationships',
    # Helpers
    'generate_and_store_embedding',
    # Constants
    'DUPLICATE_HIGH_CONFIDENCE',
    'DUPLICATE_MEDIUM_CONFIDENCE',
    'RELATIONSHIP_HIGH_CONFIDENCE',
    'RELATIONSHIP_MEDIUM_CONFIDENCE',
]
